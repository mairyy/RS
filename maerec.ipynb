{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b227fa89",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-07-26T16:55:17.227062Z",
     "iopub.status.busy": "2023-07-26T16:55:17.226604Z",
     "iopub.status.idle": "2023-07-26T16:55:17.242037Z",
     "shell.execute_reply": "2023-07-26T16:55:17.240752Z"
    },
    "papermill": {
     "duration": 0.025162,
     "end_time": "2023-07-26T16:55:17.244785",
     "exception": false,
     "start_time": "2023-07-26T16:55:17.219623",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "        \n",
    "def remove_folder_contents(folder):\n",
    "    for the_file in os.listdir(folder):\n",
    "        file_path = os.path.join(folder, the_file)\n",
    "        try:\n",
    "            if os.path.isfile(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                remove_folder_contents(file_path)\n",
    "                os.rmdir(file_path)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24329212",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-26T16:55:17.253157Z",
     "iopub.status.busy": "2023-07-26T16:55:17.252517Z",
     "iopub.status.idle": "2023-07-26T16:55:39.872496Z",
     "shell.execute_reply": "2023-07-26T16:55:39.871118Z"
    },
    "papermill": {
     "duration": 22.62746,
     "end_time": "2023-07-26T16:55:39.875623",
     "exception": false,
     "start_time": "2023-07-26T16:55:17.248163",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'MAERec'...\r\n",
      "remote: Enumerating objects: 74, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (32/32), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (26/26), done.\u001b[K\r\n",
      "remote: Total 74 (delta 15), reused 23 (delta 6), pack-reused 42\u001b[K\r\n",
      "Unpacking objects: 100% (74/74), 131.01 MiB | 7.36 MiB/s, done.\r\n",
      "Updating files: 100% (37/37), done.\r\n"
     ]
    }
   ],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "secret_value_0 = user_secrets.get_secret(\"secret_tokens\")\n",
    "\n",
    "# folder_path = '/kaggle/working/MMSRec-main'\n",
    "# remove_folder_contents(folder_path)\n",
    "# os.rmdir(folder_path)\n",
    "\n",
    "!git clone https://secret_value_0@github.com/mairyy/MAERec.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8df87c41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-26T16:55:39.890593Z",
     "iopub.status.busy": "2023-07-26T16:55:39.889676Z",
     "iopub.status.idle": "2023-07-26T16:55:39.897743Z",
     "shell.execute_reply": "2023-07-26T16:55:39.896690Z"
    },
    "papermill": {
     "duration": 0.017701,
     "end_time": "2023-07-26T16:55:39.899983",
     "exception": false,
     "start_time": "2023-07-26T16:55:39.882282",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/MAERec\n"
     ]
    }
   ],
   "source": [
    "%cd '/kaggle/working/MAERec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e06b5906",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-26T16:55:39.913805Z",
     "iopub.status.busy": "2023-07-26T16:55:39.913369Z",
     "iopub.status.idle": "2023-07-26T16:55:59.704022Z",
     "shell.execute_reply": "2023-07-26T16:55:59.702546Z"
    },
    "papermill": {
     "duration": 19.800292,
     "end_time": "2023-07-26T16:55:59.706393",
     "exception": false,
     "start_time": "2023-07-26T16:55:39.906101",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy<1.23.0,>=1.16.5\r\n",
      "  Downloading numpy-1.22.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: numpy\r\n",
      "  Attempting uninstall: numpy\r\n",
      "    Found existing installation: numpy 1.23.5\r\n",
      "    Uninstalling numpy-1.23.5:\r\n",
      "      Successfully uninstalled numpy-1.23.5\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.6 which is incompatible.\r\n",
      "chex 0.1.81 requires numpy>=1.25.0, but you have numpy 1.22.4 which is incompatible.\r\n",
      "momepy 0.6.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "pymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.22.4 which is incompatible.\r\n",
      "pymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.11.1 which is incompatible.\r\n",
      "ydata-profiling 4.3.1 requires scipy<1.11,>=1.4.1, but you have scipy 1.11.1 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed numpy-1.22.4\r\n"
     ]
    }
   ],
   "source": [
    "!pip install \"numpy>=1.16.5,<1.23.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92269ab5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-26T16:55:59.722675Z",
     "iopub.status.busy": "2023-07-26T16:55:59.722226Z",
     "iopub.status.idle": "2023-07-26T16:56:03.166201Z",
     "shell.execute_reply": "2023-07-26T16:56:03.163864Z"
    },
    "papermill": {
     "duration": 3.455342,
     "end_time": "2023-07-26T16:56:03.168661",
     "exception": false,
     "start_time": "2023-07-26T16:55:59.713319",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructed i-i graph, density=0.002261\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "# from params import args\n",
    "import numpy as np\n",
    "import pickle\n",
    "import copy\n",
    "import os\n",
    "\n",
    "def construct_graphs(seq, num_items, distance, prefix):\n",
    "    user = list()\n",
    "    r, c, d = list(), list(), list()\n",
    "    for i, seq in enumerate(seqs):\n",
    "        print(f\"Processing {i}/{len(seqs)} (>﹏<)    \", end='\\r')\n",
    "        for dist in range(1, distance + 1):\n",
    "            if dist >= len(seq): break;\n",
    "            r += copy.deepcopy(seq[+dist:])\n",
    "            c += copy.deepcopy(seq[:-dist])\n",
    "            r += copy.deepcopy(seq[:-dist])\n",
    "            c += copy.deepcopy(seq[+dist:])\n",
    "    d = np.ones_like(r)\n",
    "    iigraph = csr_matrix((d, (r, c)), shape=(num_items, num_items))\n",
    "    print('Constructed i-i graph, density=%.6f' % (len(d) / (num_items ** 2)))\n",
    "    with open(prefix + 'trn', 'wb') as fs:\n",
    "        pickle.dump(iigraph, fs)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # dataset = input('Choose a dataset: ')\n",
    "    dataset = 'clothings'\n",
    "    prefix = './datasets/' + dataset + '/'\n",
    "\n",
    "    # distance  = int(input('Max distance of edge: '))\n",
    "    distance = 3\n",
    "\n",
    "    with open(prefix + 'seq', 'rb') as fs:\n",
    "        seqs = pickle.load(fs)\n",
    "\n",
    "    if dataset == ('books'):\n",
    "        num_items = 54756\n",
    "    elif dataset == ('toys'):\n",
    "        num_items = 54784\n",
    "    elif dataset == ('retailrocket'):\n",
    "        num_items = 43886\n",
    "    elif dataset == ('sports'):\n",
    "        num_items =  18357\n",
    "    elif dataset == ('clothings'):\n",
    "        num_items = 23033\n",
    "    construct_graphs(seqs, num_items, distance, prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2add709",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-26T16:56:03.281297Z",
     "iopub.status.busy": "2023-07-26T16:56:03.280872Z",
     "iopub.status.idle": "2023-07-26T16:58:02.352664Z",
     "shell.execute_reply": "2023-07-26T16:58:02.351478Z"
    },
    "papermill": {
     "duration": 119.130878,
     "end_time": "2023-07-26T16:58:02.355442",
     "exception": false,
     "start_time": "2023-07-26T16:56:03.224564",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "# np.random.seed(12345)\n",
    "\n",
    "def sample_test_data(data_name, test_num=99, sample_type='random'):\n",
    "  \"\"\"\n",
    "  sample_type:\n",
    "      random:  sample `test_num` negative items randomly.\n",
    "      pop: sample `test_num` negative items according to item popularity.\n",
    "  \"\"\"\n",
    "\n",
    "  test_file = './datasets/' + data_name + '/tst'\n",
    "  sample_file = './datasets/' + data_name + '/neg'\n",
    "  train_file = './datasets/' + data_name + '/seq'\n",
    "\n",
    "  item_count = defaultdict(int)\n",
    "  user_items = defaultdict()\n",
    "  users = 0\n",
    "\n",
    "  #read seq in test file\n",
    "  seqs = []\n",
    "  with open(test_file, 'rb') as test:\n",
    "    seqs = pickle.load(test)\n",
    "  for seq in seqs:\n",
    "    user_items[users] = seq\n",
    "    users += 1\n",
    "    for item in seq:\n",
    "      item_count[item] += 1\n",
    "\n",
    "  #read item in train file\n",
    "  # t_seqs = []\n",
    "  # with open(train_file, 'rb') as out:\n",
    "  #   t_seqs = pickle.load(out)\n",
    "  # for seq in t_seqs:\n",
    "  #   for item in seq:\n",
    "  #     item_count[item] += 1\n",
    "\n",
    "  all_item = list(item_count.keys())\n",
    "#   print(all_item)\n",
    "  count = list(item_count.values())\n",
    "  sum_value = np.sum([x for x in count])\n",
    "  probability = [value / sum_value for value in count]\n",
    "\n",
    "  user_neg_items = defaultdict()\n",
    "#   print(len(all_item))\n",
    "  for user, user_seq in user_items.items():\n",
    "    test_samples = []\n",
    "    # print(user, \"length \", len(user_seq))\n",
    "\n",
    "    while len(test_samples) < test_num:\n",
    "#       print(len(test_samples))\n",
    "      if sample_type == 'random':\n",
    "        sample_ids = np.random.choice(all_item, test_num, replace=False)\n",
    "        # print(\"sample: \", sample_ids)\n",
    "      else: # sample_type == 'pop':\n",
    "        sample_ids = np.random.choice(all_item, test_num, replace=False, p=probability)\n",
    "\n",
    "      sample_ids = [item for item in sample_ids if item not in user_seq and item not in test_samples]\n",
    "      test_samples.extend(sample_ids)\n",
    "      # print(\"test: \", test_samples, \"length: \", len(test_samples))\n",
    "\n",
    "    test_samples = test_samples[:test_num]\n",
    "    user_neg_items[user] = test_samples\n",
    "\n",
    "  # with open(test_file, 'wb') as out:\n",
    "  l = []\n",
    "  for user, samples in user_neg_items.items():\n",
    "      l.append(samples)\n",
    "  pickle.dump(l, open(sample_file, 'wb'))\n",
    "\n",
    "# data_names = ['Beauty', 'Sports_and_Outdoors', 'Toys_and_Games', 'Yelp', 'LastFM']\n",
    "# for data_name in data_names:\n",
    "sample_test_data('clothings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f83da5b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-26T16:58:02.462702Z",
     "iopub.status.busy": "2023-07-26T16:58:02.462033Z",
     "iopub.status.idle": "2023-07-26T16:58:11.141174Z",
     "shell.execute_reply": "2023-07-26T16:58:11.139884Z"
    },
    "papermill": {
     "duration": 8.735082,
     "end_time": "2023-07-26T16:58:11.143939",
     "exception": false,
     "start_time": "2023-07-26T16:58:02.408857",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(lr=0.001, seed=19260817, data='clothings', epoch=20, trn_batch=256, tst_batch=256, con_batch=2048, test_frequency=1, max_seq_len=50, num_reco_neg=40, reg=1e-06, ssl_reg=0.01, latdim=32, mask_depth=3, path_prob=0.5, num_attention_heads=4, num_gcn_layers=2, num_trm_layers=2, load_model=None, num_mask_cand=50, mask_steps=10, eps=0.2, attention_probs_dropout_prob=0.3, hidden_dropout_prob=0.3, save_path='tem')\r\n",
      "2023-07-26 16:58:07.163892: Start\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/kaggle/working/MAERec/main.py\", line 319, in <module>\r\n",
      "    handler.load_data()\r\n",
      "  File \"/kaggle/working/MAERec/handler.py\", line 68, in load_data\r\n",
      "    self.ii_adj = self.make_torch_adj(trn)\r\n",
      "  File \"/kaggle/working/MAERec/handler.py\", line 52, in make_torch_adj\r\n",
      "    return t.sparse.FloatTensor(idxs, vals, shape).cuda()\r\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 239, in _lazy_init\r\n",
      "    raise AssertionError(\"Torch not compiled with CUDA enabled\")\r\n",
      "AssertionError: Torch not compiled with CUDA enabled\r\n"
     ]
    }
   ],
   "source": [
    "# !sudo apt install python3-scipy\n",
    "!python main.py --data clothings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df81b9e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-26T16:58:11.251789Z",
     "iopub.status.busy": "2023-07-26T16:58:11.251362Z",
     "iopub.status.idle": "2023-07-26T16:58:12.920907Z",
     "shell.execute_reply": "2023-07-26T16:58:12.919861Z"
    },
    "papermill": {
     "duration": 1.724514,
     "end_time": "2023-07-26T16:58:12.923185",
     "exception": false,
     "start_time": "2023-07-26T16:58:11.198671",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39387\n",
      "23032\n",
      "[11, 12, 13, 14]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from torch import nn\n",
    "\n",
    "def feature(path):\n",
    "    with open(path, 'rb') as out:\n",
    "        feature = pickle.load(out)\n",
    "    return feature\n",
    "\n",
    "f = feature('./datasets/clothings/tst')\n",
    "print(len(f))\n",
    "max_len = 0\n",
    "for i in f:\n",
    "    for line in i:\n",
    "        if line > max_len:\n",
    "            max_len = line\n",
    "print(max_len)\n",
    "print(f[1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 187.096682,
   "end_time": "2023-07-26T16:58:14.400830",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-07-26T16:55:07.304148",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
